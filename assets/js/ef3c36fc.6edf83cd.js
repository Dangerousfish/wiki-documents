"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[18530],{15680:(e,t,n)=>{n.d(t,{xA:()=>c,yg:()=>m});var r=n(96540);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var p=r.createContext({}),l=function(e){var t=r.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},c=function(e){var t=l(e.components);return r.createElement(p.Provider,{value:t},e.children)},d="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=l(n),u=o,m=d["".concat(p,".").concat(u)]||d[u]||g[u]||i;return n?r.createElement(m,a(a({ref:t},c),{},{components:n})):r.createElement(m,a({ref:t},c))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,a=new Array(i);a[0]=u;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[d]="string"==typeof e?e:o,a[1]=s;for(var l=2;l<i;l++)a[l]=n[l];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},92713:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>a,default:()=>g,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var r=n(9668),o=(n(96540),n(15680));const i={description:"This wiki will demonstrate how you can use TensorFlow Lite for keyword spotting with ReSpeaker 2-Mics Pi HAT v2 and perform speech recognition.",title:"Keyword Spotting with TensorFlow Lite",keywords:["ReSpeaker_2-Mics_Pi_HAT","Keyword_Spotting","TensorFlow_Lite"],image:"https://files.seeedstudio.com/wiki/ReSpeaker_2_Mics_Pi_HAT/social-image.webp",slug:"/respeaker_2_mics_pi_hat_v2_speech_recognition",last_update:{date:"12/23/2024",author:"Joshua Lee"}},a=void 0,s={unversionedId:"Sensor/ReSpeaker_2-Mics_Pi_HAT/v2/ReSpeaker_2_Mics_Pi_Hat_v2_Speech_Recognition",id:"Sensor/ReSpeaker_2-Mics_Pi_HAT/v2/ReSpeaker_2_Mics_Pi_Hat_v2_Speech_Recognition",title:"Keyword Spotting with TensorFlow Lite",description:"This wiki will demonstrate how you can use TensorFlow Lite for keyword spotting with ReSpeaker 2-Mics Pi HAT v2 and perform speech recognition.",source:"@site/docs/Sensor/ReSpeaker_2-Mics_Pi_HAT/v2/ReSpeaker_2_Mics_Pi_Hat_v2_Speech_Recognition.md",sourceDirName:"Sensor/ReSpeaker_2-Mics_Pi_HAT/v2",slug:"/respeaker_2_mics_pi_hat_v2_speech_recognition",permalink:"/respeaker_2_mics_pi_hat_v2_speech_recognition",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Sensor/ReSpeaker_2-Mics_Pi_HAT/v2/ReSpeaker_2_Mics_Pi_Hat_v2_Speech_Recognition.md",tags:[],version:"current",lastUpdatedBy:"Joshua Lee",lastUpdatedAt:1734912e3,formattedLastUpdatedAt:"Dec 23, 2024",frontMatter:{description:"This wiki will demonstrate how you can use TensorFlow Lite for keyword spotting with ReSpeaker 2-Mics Pi HAT v2 and perform speech recognition.",title:"Keyword Spotting with TensorFlow Lite",keywords:["ReSpeaker_2-Mics_Pi_HAT","Keyword_Spotting","TensorFlow_Lite"],image:"https://files.seeedstudio.com/wiki/ReSpeaker_2_Mics_Pi_HAT/social-image.webp",slug:"/respeaker_2_mics_pi_hat_v2_speech_recognition",last_update:{date:"12/23/2024",author:"Joshua Lee"}}},p={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Hardware and Software Requirements",id:"hardware-and-software-requirements",level:3},{value:"Applications",id:"applications",level:3},{value:"What is TensorFlow Lite?",id:"what-is-tensorflow-lite",level:3},{value:"Train and Get TensorFlow Lite Model",id:"train-and-get-tensorflow-lite-model",level:2},{value:"Dataset",id:"dataset",level:3},{value:"Why Use Google Colab?",id:"why-use-google-colab",level:3},{value:"Steps",id:"steps",level:3},{value:"Local Inference",id:"local-inference",level:2},{value:"Running the Inference Script",id:"running-the-inference-script",level:3},{value:"Steps to Run",id:"steps-to-run",level:3},{value:"Interpreting the Results",id:"interpreting-the-results",level:3},{value:"Tech Support &amp; Product Discussion",id:"tech-support--product-discussion",level:2}],c={toc:l},d="wrapper";function g(e){let{components:t,...n}=e;return(0,o.yg)(d,(0,r.A)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("h2",{id:"introduction"},"Introduction"),(0,o.yg)("p",null,"This project demonstrates how to use TensorFlow Lite for keyword spotting on the ReSpeaker 2-Mics Pi HAT v2. Keyword spotting allows for real-time detection of predefined words from audio input, enabling applications such as voice-controlled devices and interactive systems. We will guide you through the steps to train a TensorFlow Lite model, deploy it on the ReSpeaker HAT, and run speech recognition locally."),(0,o.yg)("h3",{id:"hardware-and-software-requirements"},"Hardware and Software Requirements"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Hardware: Raspberry Pi with ReSpeaker 2-Mics Pi HAT v2"),(0,o.yg)("li",{parentName:"ul"},"Software: TensorFlow Lite, Google Colab, Python, and supporting libraries")),(0,o.yg)("h3",{id:"applications"},"Applications"),(0,o.yg)("p",null,"Keyword spotting can be applied in:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Smart home devices"),(0,o.yg)("li",{parentName:"ul"},"Voice-controlled robots"),(0,o.yg)("li",{parentName:"ul"},"Interactive kiosks")),(0,o.yg)("h3",{id:"what-is-tensorflow-lite"},"What is TensorFlow Lite?"),(0,o.yg)("p",null,"TensorFlow Lite is a lightweight version of TensorFlow designed for mobile and embedded devices. It enables machine learning inference with low latency and small binary sizes, making it ideal for running models on edge devices like Raspberry Pi."),(0,o.yg)("h2",{id:"train-and-get-tensorflow-lite-model"},"Train and Get TensorFlow Lite Model"),(0,o.yg)("h3",{id:"dataset"},"Dataset"),(0,o.yg)("p",null,"We will use a subset of the Speech Commands dataset for training. The dataset contains WAV audio files of people saying different words, collected by Google and released under a CC BY license. The dataset can be downloaded from here. For more information on datasets, refer to this guide."),(0,o.yg)("h3",{id:"why-use-google-colab"},"Why Use Google Colab?"),(0,o.yg)("p",null,"Google Colab is a cloud-based platform for running Jupyter notebooks. It provides free access to GPU resources, making it an excellent choice for training machine learning models without requiring local computation power."),(0,o.yg)("h3",{id:"steps"},"Steps"),(0,o.yg)("p",null,"Now we will use a Google Colab Notebook to perform the data training and generate a TensorFlow Lite model in ",(0,o.yg)("inlineCode",{parentName:"p"},".tflite")," format."),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},"Step 1.")," Open ",(0,o.yg)("a",{parentName:"p",href:"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb"},"this Python Notebook")),(0,o.yg)("p",{parentName:"li"},(0,o.yg)("img",{parentName:"p",src:"https://files.seeedstudio.com/wiki/ReSpeaker_2_Mics_Pi_HAT/v2/speech_1.png",alt:"Load Notebook"})),(0,o.yg)("p",{parentName:"li"},"By default, it will load ",(0,o.yg)("a",{parentName:"p",href:"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip"},"the mini Speech Commands dataset")," which is a smaller version of the Speech Commands dataset. The original dataset consists of over 105,000 audio files in the WAV (Waveform) audio file format of people saying 35 different words. This data was collected by Google and released under a CC BY license.")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},"Step 2.")," Connect to a new runtime by selecting ",(0,o.yg)("strong",{parentName:"p"},"Changing runtime type -> CPU -> Save"),", then click ",(0,o.yg)("strong",{parentName:"p"},"Connect"),"."),(0,o.yg)("p",{parentName:"li"},(0,o.yg)("img",{parentName:"p",src:"https://files.seeedstudio.com/wiki/ReSpeaker_2_Mics_Pi_HAT/v2/speech_2.png",alt:"Change runtime type - 1"}),"\n",(0,o.yg)("img",{parentName:"p",src:"https://files.seeedstudio.com/wiki/ReSpeaker_2_Mics_Pi_HAT/v2/speech_3.png",alt:"Change runtime type - 2"}))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},"Step 3.")," Navigate to ",(0,o.yg)("inlineCode",{parentName:"p"},"Runtime > Run all")," to run all the code cells. This process will take about 10 minutes to complete."),(0,o.yg)("p",{parentName:"li"},(0,o.yg)("img",{parentName:"p",src:"https://files.seeedstudio.com/wiki/XIAO-BLE/TFLite/pics/micro-speech-run-all.png",alt:"Run all"}))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},"Step 4.")," Once all the code cells are executed, append a new cell and run the following code to generate the ",(0,o.yg)("inlineCode",{parentName:"p"},".tflite")," model file."),(0,o.yg)("pre",{parentName:"li"},(0,o.yg)("code",{parentName:"pre",className:"language-python"},"converter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\nwith open('model.tflite', 'wb') as f:\nf.write(tflite_model)\n")),(0,o.yg)("p",{parentName:"li"},(0,o.yg)("img",{parentName:"p",src:"https://files.seeedstudio.com/wiki/ReSpeaker_2_Mics_Pi_HAT/v2/speech_4.png",alt:"Append a new cell"}))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("strong",{parentName:"p"},"Step 5.")," Right click the generated ",(0,o.yg)("inlineCode",{parentName:"p"},"model.tflite")," file and select ",(0,o.yg)("strong",{parentName:"p"},"Download")," to save the file to your computer."),(0,o.yg)("p",{parentName:"li"},(0,o.yg)("img",{parentName:"p",src:"https://files.seeedstudio.com/wiki/ReSpeaker_2_Mics_Pi_HAT/v2/speech_5.png",alt:"Download"})))),(0,o.yg)("h2",{id:"local-inference"},"Local Inference"),(0,o.yg)("h3",{id:"running-the-inference-script"},"Running the Inference Script"),(0,o.yg)("p",null,"The script inference.py performs the following steps:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},"Loads the trained TensorFlow Lite model."),(0,o.yg)("li",{parentName:"ol"},"Processes input audio into a spectrogram suitable for inference."),(0,o.yg)("li",{parentName:"ol"},"Runs the inference and outputs the detected keyword along with confidence scores for each label.")),(0,o.yg)("h3",{id:"steps-to-run"},"Steps to Run"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"Upload the ",(0,o.yg)("inlineCode",{parentName:"p"},"model.tflite")," model file to your Pi, in this example, we put it in ",(0,o.yg)("inlineCode",{parentName:"p"},"~/speech_recognition/model.tflite"),".")),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"Save the following script as ",(0,o.yg)("inlineCode",{parentName:"p"},"~/speech_recognition/inference.py"),":"),(0,o.yg)("pre",{parentName:"li"},(0,o.yg)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nfrom scipy import signal\nfrom tflite_runtime.interpreter import Interpreter\nimport soundfile as sf\n\nMODEL_PATH = 'model.tflite'\nLABELS = ['no', 'yes', 'down', 'go', 'left', 'up', 'right', 'stop']\n")))),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"def get_spectrogram(waveform, expected_time_steps=124, expected_freq_bins=129):\n    _, _, Zxx = signal.stft(\n        waveform,\n        fs=16000,\n        nperseg=255,\n        noverlap=124,\n        nfft=256\n    )\n    spectrogram = np.abs(Zxx)\n\n    if spectrogram.shape[0] != expected_freq_bins:\n        spectrogram = np.pad(spectrogram, ((\n            0, expected_freq_bins - spectrogram.shape[0]), (0, 0)), mode='constant')\n    if spectrogram.shape[1] != expected_time_steps:\n        spectrogram = np.pad(spectrogram, ((\n            0, 0), (0, expected_time_steps - spectrogram.shape[1])), mode='constant')\n\n    if spectrogram.shape != (expected_freq_bins, expected_time_steps):\n        raise ValueError(\n            f\"Invalid spectrogram shape. Got {spectrogram.shape}, expected ({expected_freq_bins}, {expected_time_steps}).\"\n        )\n\n    spectrogram = np.transpose(spectrogram)\n\n    return spectrogram\n\n\ndef preprocess_audio(file_path):\n    waveform, sample_rate = sf.read(file_path)\n    if sample_rate != 16000:\n        raise ValueError(\"Expected sample rate is 16 kHz\")\n\n    if len(waveform.shape) > 1:\n        waveform = waveform[:, 0]\n\n    spectrogram = get_spectrogram(waveform)\n    spectrogram = spectrogram[..., np.newaxis]\n    spectrogram = spectrogram[np.newaxis, ...]\n\n    return spectrogram\n\n\ndef run_inference(file_path):\n    spectrogram = preprocess_audio(file_path)\n\n    interpreter = Interpreter(MODEL_PATH)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_shape = input_details[0]['shape']\n    if spectrogram.shape != tuple(input_shape):\n        raise ValueError(\n            f\"Expected input shape {input_shape}, got {spectrogram.shape}\"\n        )\n\n    interpreter.set_tensor(\n        input_details[0]['index'], spectrogram.astype(np.float32))\n\n    interpreter.invoke()\n\n    output_data = interpreter.get_tensor(output_details[0]['index'])[0]\n    prediction = np.argmax(output_data)\n    confidence = np.exp(output_data) / \\\n        np.sum(np.exp(output_data))\n\n    print(f\"command: {LABELS[prediction].upper()}\")\n    for label, conf in zip(LABELS, confidence):\n        print(f\"{label}: {conf:.2%}\")\n\n\nif __name__ == \"__main__\":\n    audio_file_path = 'test_audio.wav'\n    run_inference(audio_file_path)\n```\n")),(0,o.yg)("ol",{start:3},(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"Record a sound using the following command, available keywords are: ",(0,o.yg)("inlineCode",{parentName:"p"},"no"),", ",(0,o.yg)("inlineCode",{parentName:"p"},"yes"),", ",(0,o.yg)("inlineCode",{parentName:"p"},"down"),", ",(0,o.yg)("inlineCode",{parentName:"p"},"go"),", ",(0,o.yg)("inlineCode",{parentName:"p"},"left"),", ",(0,o.yg)("inlineCode",{parentName:"p"},"up"),", ",(0,o.yg)("inlineCode",{parentName:"p"},"right"),", ",(0,o.yg)("inlineCode",{parentName:"p"},"stop"),"."),(0,o.yg)("pre",{parentName:"li"},(0,o.yg)("code",{parentName:"pre"},'$ arecord -D "plughw:2,0" -f S16_LE -r 16000 -d 1 -t wav ~/speech_recognition/test_audio.wav\n'))),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("p",{parentName:"li"},"Execute the script:"),(0,o.yg)("pre",{parentName:"li"},(0,o.yg)("code",{parentName:"pre"},"$ python3 inference.py\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\ncommand: YES\nno: 8.74%\nyes: 21.10%\ndown: 5.85%\ngo: 14.57%\nleft: 11.02%\nup: 8.25%\nright: 10.53%\nstop: 19.94%\n")))),(0,o.yg)("h3",{id:"interpreting-the-results"},"Interpreting the Results"),(0,o.yg)("p",null,"The script outputs the detected command (e.g., YES) and the confidence scores for all labels. This provides insights into the model\u2019s predictions and allows you to evaluate its performance."),(0,o.yg)("h2",{id:"tech-support--product-discussion"},"Tech Support & Product Discussion"),(0,o.yg)("p",null,"Thank you for choosing our products! We are here to provide you with different support to ensure that your experience with our products is as smooth as possible. We offer several communication channels to cater to different preferences and needs."),(0,o.yg)("div",{class:"button_tech_support_container"},(0,o.yg)("a",{href:"https://forum.seeedstudio.com/",class:"button_forum"}),(0,o.yg)("a",{href:"https://www.seeedstudio.com/contacts",class:"button_email"})),(0,o.yg)("div",{class:"button_tech_support_container"},(0,o.yg)("a",{href:"https://discord.gg/eWkprNDMU7",class:"button_discord"}),(0,o.yg)("a",{href:"https://github.com/Seeed-Studio/wiki-documents/discussions/69",class:"button_discussion"})))}g.isMDXComponent=!0}}]);