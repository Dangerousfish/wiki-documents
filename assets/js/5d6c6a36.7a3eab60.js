"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[32273],{15680:(e,t,o)=>{o.d(t,{xA:()=>s,yg:()=>c});var n=o(96540);function i(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function a(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,n)}return o}function r(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?a(Object(o),!0).forEach((function(t){i(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):a(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function l(e,t){if(null==e)return{};var o,n,i=function(e,t){if(null==e)return{};var o,n,i={},a=Object.keys(e);for(n=0;n<a.length;n++)o=a[n],t.indexOf(o)>=0||(i[o]=e[o]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)o=a[n],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(i[o]=e[o])}return i}var d=n.createContext({}),p=function(e){var t=n.useContext(d),o=t;return e&&(o="function"==typeof e?e(t):r(r({},t),e)),o},s=function(e){var t=p(e.components);return n.createElement(d.Provider,{value:t},e.children)},g="mdxType",y={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var o=e.components,i=e.mdxType,a=e.originalType,d=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),g=p(o),m=i,c=g["".concat(d,".").concat(m)]||g[m]||y[m]||a;return o?n.createElement(c,r(r({ref:t},s),{},{components:o})):n.createElement(c,r({ref:t},s))}));function c(e,t){var o=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=o.length,r=new Array(a);r[0]=m;var l={};for(var d in t)hasOwnProperty.call(t,d)&&(l[d]=t[d]);l.originalType=e,l[g]="string"==typeof e?e:i,r[1]=l;for(var p=2;p<a;p++)r[p]=o[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,o)}m.displayName="MDXCreateElement"},15803:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>y,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var n=o(9668),i=(o(96540),o(15680));const a={description:"Convert and Quantize AI Models for reCamera",title:"Model Conversion Guide",keywords:["Edge","reCamera"],image:"https://files.seeedstudio.com/wiki/reCamera/013.jpg",slug:"/recamera_model_conversion",last_update:{date:"10/18/2024",author:"Dayu Li"}},r="Convert and Quantize AI Models",l={unversionedId:"Edge/reCamera/reCamera 2002 Series/overview_of_ai_model_deployment/reCamera_model_conversion",id:"Edge/reCamera/reCamera 2002 Series/overview_of_ai_model_deployment/reCamera_model_conversion",title:"Model Conversion Guide",description:"Convert and Quantize AI Models for reCamera",source:"@site/docs/Edge/reCamera/reCamera 2002 Series/overview_of_ai_model_deployment/reCamera_model_conversion.md",sourceDirName:"Edge/reCamera/reCamera 2002 Series/overview_of_ai_model_deployment",slug:"/recamera_model_conversion",permalink:"/recamera_model_conversion",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Edge/reCamera/reCamera 2002 Series/overview_of_ai_model_deployment/reCamera_model_conversion.md",tags:[],version:"current",lastUpdatedBy:"Dayu Li",lastUpdatedAt:1729209600,formattedLastUpdatedAt:"Oct 18, 2024",frontMatter:{description:"Convert and Quantize AI Models for reCamera",title:"Model Conversion Guide",keywords:["Edge","reCamera"],image:"https://files.seeedstudio.com/wiki/reCamera/013.jpg",slug:"/recamera_model_conversion",last_update:{date:"10/18/2024",author:"Dayu Li"}},sidebar:"ProductSidebar",previous:{title:"Overview of AI model Deployment",permalink:"/recamera_ai_model_deployment"},next:{title:"Others",permalink:"/recamera_others"}},d={},p=[{value:"Set up the working environment",id:"set-up-the-working-environment",level:2},{value:"Method 1: Local Installation",id:"method-1-local-installation",level:3},{value:"Method 2: Installation in a Docker Image (for Incompatible Local Environments)",id:"method-2-installation-in-a-docker-image-for-incompatible-local-environments",level:3},{value:"Convert and Quantize AI Models to the cvimodel Format",id:"convert-and-quantize-ai-models-to-the-cvimodel-format",level:2},{value:"Preparing the Workspace",id:"preparing-the-workspace",level:3},{value:"ONNX to MLIR",id:"onnx-to-mlir",level:3},{value:"MLIR to F16 cvimodel",id:"mlir-to-f16-cvimodel",level:3},{value:"MLIR to INT8 cvimodel",id:"mlir-to-int8-cvimodel",level:3},{value:"Calibration table generation",id:"calibration-table-generation",level:4},{value:"Compile to INT8 symmetric quantized cvimodel",id:"compile-to-int8-symmetric-quantized-cvimodel",level:4},{value:"Quick Test",id:"quick-test",level:3},{value:"Mixed-Precision Quantization",id:"mixed-precision-quantization",level:3},{value:"Resources",id:"resources",level:2},{value:"Tech Support &amp; Product Discussion",id:"tech-support--product-discussion",level:2}],s={toc:p},g="wrapper";function y(e){let{components:t,...o}=e;return(0,i.yg)(g,(0,n.A)({},s,o,{components:t,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"convert-and-quantize-ai-models"},"Convert and Quantize AI Models"),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:600,src:"https://files.seeedstudio.com/wiki/reCamera/013.jpg"})),(0,i.yg)("p",null,"The AI model conversion tool of ",(0,i.yg)("strong",{parentName:"p"},"reCamera")," currently supports frameworks such as ",(0,i.yg)("inlineCode",{parentName:"p"},"PyTorch"),", ",(0,i.yg)("inlineCode",{parentName:"p"},"ONNX"),", ",(0,i.yg)("inlineCode",{parentName:"p"},"TFLite"),", and ",(0,i.yg)("inlineCode",{parentName:"p"},"Caffe"),". Models from other frameworks need to be converted into ",(0,i.yg)("inlineCode",{parentName:"p"},"ONNX")," format. For instructions on how to convert models from other deep learning architectures to ",(0,i.yg)("inlineCode",{parentName:"p"},"ONNX"),", you can refer to the official ONNX website: ",(0,i.yg)("a",{parentName:"p",href:"https://github.com/onnx/tutorials"},"https://github.com/onnx/tutorials"),"."),(0,i.yg)("p",null,"The flow diagram for deploying AI models on ",(0,i.yg)("strong",{parentName:"p"},"reCamera")," is shown below."),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:600,src:"https://files.seeedstudio.com/wiki/reCamera/InferToolchain_Arch.png"})),"This article introduces how to use reCamera's AI model conversion tool through simple examples.",(0,i.yg)("h2",{id:"set-up-the-working-environment"},"Set up the working environment"),(0,i.yg)("h3",{id:"method-1-local-installation"},"Method 1: Local Installation"),(0,i.yg)("p",null,"First check whether the current system environment meets:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"#jump1"},"Ubuntu 22.04")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"#jump2"},"Python 3.10"))),(0,i.yg)("p",null,"If it is not satisfied or the installation fails, choose ",(0,i.yg)("inlineCode",{parentName:"p"},"Method 2")," to install the model conversion tool."),(0,i.yg)("p",null,"Install ",(0,i.yg)("inlineCode",{parentName:"p"},"tpu_mlir")," using ",(0,i.yg)("inlineCode",{parentName:"p"},"pip"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"pip install tpu_mlir\n")),(0,i.yg)("p",null,"The dependencies required by ",(0,i.yg)("inlineCode",{parentName:"p"},"tpu_mlir")," vary when handling models from different frameworks. For model files generated by ONNX or Torch, install the additional dependencies using the following command:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"pip install tpu_mlir[onnx]\npip install tpu_mlir[torch]\n")),(0,i.yg)("p",null,"Currently, five configurations are supported: onnx, torch, tensorflow, caffe, and paddle. Or you can use the following command to install all dependencies:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"pip install tpu_mlir[all]\n")),(0,i.yg)("p",null,"When the tpu_mlir-{version}.whl file already exists locally, you can also use the following command to install it:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"pip install path/to/tpu_mlir-{version}.whl[all]\n")),(0,i.yg)("h3",{id:"method-2-installation-in-a-docker-image-for-incompatible-local-environments"},"Method 2: Installation in a Docker Image (for Incompatible Local Environments)"),(0,i.yg)("p",null,"Download the required image from ",(0,i.yg)("a",{parentName:"p",href:"https://hub.docker.com/r/sophgo/tpuc_dev"},"DockerHub (click here)")," and enter a command similar to the following:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"docker pull sophgo/tpuc_dev:v3.1\n")),(0,i.yg)("p",null,"If you are using Docker for the first time, you can run the following commands for installation and configuration (only needed for the first-time setup):"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"sudo apt install docker.io\nsudo systemctl start docker\nsudo systemctl enable docker\nsudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker\n")),(0,i.yg)("p",null,"Make sure the installation package is in the current directory, and then create a container in the current directory as follows:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"docker run --privileged --name MyName -v $PWD:/workspace -it sophgo/tpuc_dev:v3.1\n")),(0,i.yg)("p",null,"*",(0,i.yg)("em",{parentName:"p"}," Replace ",(0,i.yg)("inlineCode",{parentName:"em"},'"MyName"')," with the desired name for your container")),(0,i.yg)("p",null,"Use ",(0,i.yg)("inlineCode",{parentName:"p"},"pip")," to install ",(0,i.yg)("inlineCode",{parentName:"p"},"tpu_mlir")," in the /workspace directory inside the Docker container, just like in ",(0,i.yg)("inlineCode",{parentName:"p"},"Method 1"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"pip install tpu_mlir[all]\n")),(0,i.yg)("h2",{id:"convert-and-quantize-ai-models-to-the-cvimodel-format"},"Convert and Quantize AI Models to the cvimodel Format"),(0,i.yg)("p",null,"reCamera has already adapted the YOLO series for local inference. Therefore, this section uses ",(0,i.yg)("inlineCode",{parentName:"p"},"yolo11n.onnx")," as an example to demonstrate how to convert an ONNX model to the ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel"),".\n",(0,i.yg)("strong",{parentName:"p"},"The ",(0,i.yg)("inlineCode",{parentName:"strong"},"cvimodel")," is the AI model format used for local inference on reCamera.")),(0,i.yg)("p",null,"The method for converting and quantizing PyTorch, TFLite, and Caffe models is the same as that in this section."),(0,i.yg)("p",null,"Here is the download link for yolo11n.onnx. You can click the link to download the model and copy it to your workspace for further use."),(0,i.yg)("p",null,"Download the model:",(0,i.yg)("br",{parentName:"p"}),"\n",(0,i.yg)("a",{parentName:"p",href:"https://seeedstudio88-my.sharepoint.com/:u:/g/personal/youjiang_yu_seeedstudio88_onmicrosoft_com/ESj2_zJM4oxOiv62Hh1XKu8BA9gWPQy6zAGSXWd4VL--9w?e=tagPRA"},"Download yolo11n.onnx")),(0,i.yg)("p",null,"After downloading the model, please place it into your ",(0,i.yg)("inlineCode",{parentName:"p"},"workspace")," for the next steps."),(0,i.yg)("h3",{id:"preparing-the-workspace"},"Preparing the Workspace"),(0,i.yg)("p",null,"Create the ",(0,i.yg)("inlineCode",{parentName:"p"},"model_yolo11n")," directory at the same level as ",(0,i.yg)("inlineCode",{parentName:"p"},"tpu-mlir"),", and place both the model file and image files inside the ",(0,i.yg)("inlineCode",{parentName:"p"},"model_yolo11n")," directory. The image files are usually part of the model\u2019s training dataset, used for calibration during the subsequent quantization process.\nEnter the following command in the terminal:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/sophgo/tpu-mlir.git\ncd tpu-mlir\nsource ./envsetup.sh\n./build.sh\nmkdir model_yolo11n && cd model_yolo11n\ncp -rf ${REGRESSION_PATH}/dataset/COCO2017 .\ncp -rf ${REGRESSION_PATH}/image .\nmkdir workspace && cd workspace\n")),(0,i.yg)("h3",{id:"onnx-to-mlir"},"ONNX to MLIR"),(0,i.yg)("p",null,"The conversion from ONNX to ",(0,i.yg)("inlineCode",{parentName:"p"},"MLIR")," is an intermediate step in the model transformation process. Before obtaining a model suitable for inference on reCamera, you need to first convert the ONNX model to the ",(0,i.yg)("inlineCode",{parentName:"p"},"MLIR")," format. This ",(0,i.yg)("inlineCode",{parentName:"p"},"MLIR")," file serves as a bridge to generate the final model optimized for reCamera's inference engine."),(0,i.yg)("p",null,"If the input is image, we need to know the preprocessing of the model before transferring it. If\nthe model uses preprocessed npz files as input, no preprocessing needs to be considered. The\npreprocessing process is formulated as follows ( ",(0,i.yg)("inlineCode",{parentName:"p"},"x")," represents the input):"),(0,i.yg)("div",{align:"center"},(0,i.yg)("p",null,"y = (x \u2212 mean) \xd7 scale")),(0,i.yg)("p",null,"The normalization range of yolo11 is ",(0,i.yg)("strong",{parentName:"p"},"[0, 1]"),", and the image of the official yolo11 is RGB. Each value will be multiplied by ",(0,i.yg)("strong",{parentName:"p"},"1/255"),", respectively\ncorresponding to ",(0,i.yg)("strong",{parentName:"p"},"0.0, 0.0, 0.0")," and ",(0,i.yg)("strong",{parentName:"p"},"0.0039216, 0.0039216, 0.0039216")," when it is converted into ",(0,i.yg)("inlineCode",{parentName:"p"},"mean"),"\nand ",(0,i.yg)("inlineCode",{parentName:"p"},"scale"),". The parameters for ",(0,i.yg)("inlineCode",{parentName:"p"},"mean")," and ",(0,i.yg)("inlineCode",{parentName:"p"},"scale")," differ depending on the model, as they are determined by the normalization method used for each specific model."),(0,i.yg)("p",null,"You can refer to the following model conversion command in terminal:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},'model_transform \\\n  --model_name yolo11n \\\n  --model_def yolo11n.onnx \\\n  --input_shapes "[[1,3,640,640]]" \\\n  --mean "0.0,0.0,0.0" \\\n  --scale "0.0039216,0.0039216,0.0039216" \\\n  --keep_aspect_ratio \\\n  --pixel_format rgb \\\n  --output_names "/model.23/cv2.0/cv2.0.2/Conv_output_0,/model.23/cv3.0/cv3.0.2/Conv_output_0,/model.23/cv2.1/cv2.1.2/Conv_output_0,/model.23/cv3.1/cv3.1.2/Conv_output_0,/model.23/cv2.2/cv2.2.2/Conv_output_0,/model.23/cv3.2/cv3.2.2/Conv_output_0" \\\n  --test_input ../image/dog.jpg \\\n  --test_result yolo11n_top_outputs.npz \\\n  --mlir yolo11n.mlir\n')),(0,i.yg)("p",null,"After converting to an ",(0,i.yg)("inlineCode",{parentName:"p"},"mlir")," file, a ",(0,i.yg)("inlineCode",{parentName:"p"},"${model_name}_in_f32.npz")," file will be generated, which\nis the input file for the subsequent models."),(0,i.yg)("p",null,"Regarding the selection of the ",(0,i.yg)("inlineCode",{parentName:"p"},"--output_names")," parameter, the YOLO11 model conversion in this example does not choose the final output named output0. Instead, it selects the six outputs before the model's head as the parameter. You can import the ",(0,i.yg)("inlineCode",{parentName:"p"},"ONNX")," file into ",(0,i.yg)("a",{parentName:"p",href:"https://netron.app/"},"Netron")," to view the model structure."),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:600,src:"https://files.seeedstudio.com/wiki/reCamera/recamera_model_conversion04.png"})),(0,i.yg)("p",null,"The operators in the YOLO's ",(0,i.yg)("inlineCode",{parentName:"p"},"head")," have very low accuracy after ",(0,i.yg)("strong",{parentName:"p"},"INT8")," quantization. If ",(0,i.yg)("inlineCode",{parentName:"p"},"output0")," at the very end were chosen as the parameter, mixed-precision quantization would be required."),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Since the subsequent sections of this article will provide examples of mixed-precision quantization, and this section uses a single quantization precision for the example"),", the outputs before the ",(0,i.yg)("inlineCode",{parentName:"p"},"head")," are chosen as parameters. By visualizing the ONNX model in ",(0,i.yg)("a",{parentName:"p",href:"https://netron.app/"},"Netron"),", you can see the positions of the six output names:"),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:"400",src:"https://files.seeedstudio.com/wiki/reCamera/recamera_model_conversion02.png"})),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:"400",src:"https://files.seeedstudio.com/wiki/reCamera/recamera_model_conversion.01png"})),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:"400",src:"https://files.seeedstudio.com/wiki/reCamera/recamera_model_conversion02.png"})),(0,i.yg)("p",null,"Description of Main Parameters for ",(0,i.yg)("inlineCode",{parentName:"p"},"model_transform"),":"),(0,i.yg)("table",{style:{width:"80%",fontSize:"14px",borderCollapse:"collapse",margin:"20px auto"}},(0,i.yg)("thead",null,(0,i.yg)("tr",null,(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Parameter Name"),(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Required?"),(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Description"))),(0,i.yg)("tbody",null,(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"model_name"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the model name.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"model_def"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the model definition file, such as '.onnx', '.tflite', or '.prototxt'.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"input_shapes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the input shape, e.g., [[1,3,640,640]]. A two-dimensional array that can support multiple inputs.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"input_types"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the input types, such as int32. Use commas to separate multiple inputs. Default is float32.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"resize_dims"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the dimensions to which the original image should be resized. If not specified, it will be resized to the model's input size.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"keep_aspect_ratio"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Whether to keep the aspect ratio when resizing. Default is false; if true, padding with zeros will be used for missing areas.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"mean"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Mean value for each channel of the image. Default is 0,0,0,0.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"scale"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Scale value for each channel of the image. Default is 1.0,1.0,1.0.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"pixel_format"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Image type, which can be one of 'rgb', 'bgr', 'gray', or 'rgbd'. Default is 'bgr'.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"channel_format"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Channel type for image input, which can be 'nhwc' or 'nchw'. For non-image inputs, use 'none'. Default is 'nchw'.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"output_names"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify output names. If not specified, the model's default output names are used.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"test_input"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify an input file for validation, such as an image, npy, or npz file. If not specified, no accuracy validation is performed.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"test_result"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the output file for the validation result.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"excepts"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify network layers to exclude from validation, separated by commas.")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"mlir"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the output MLIR file name and path.")))),(0,i.yg)("h3",{id:"mlir-to-f16-cvimodel"},"MLIR to F16 cvimodel"),(0,i.yg)("p",null,"If you want to convert from ",(0,i.yg)("inlineCode",{parentName:"p"},"mlir")," to F16-precision ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel"),", you can enter the following reference command in the terminal:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"model_deploy \\\n  --mlir yolo11n.mlir \\\n  --quantize F16 \\\n  --processor cv181x \\\n  --test_input yolo11n_in_f32.npz \\\n  --test_reference yolo11n_top_outputs.npz \\\n  --fuse_preprocess \\\n  --tolerance 0.99,0.99 \\\n  --model yolo11n_1684x_f16.cvimodel\n")),(0,i.yg)("p",null,"After a successful conversion, you will obtain an FP16-precision ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel")," file that can be directly used for inference. If you need an ",(0,i.yg)("strong",{parentName:"p"},"INT8-precision")," or ",(0,i.yg)("strong",{parentName:"p"},"mixed-precision")," ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel")," file, please refer to the content in the later sections of the following article."),(0,i.yg)("p",null,"Description of Main Parameters for ",(0,i.yg)("inlineCode",{parentName:"p"},"model_deploy"),":"),(0,i.yg)("table",{style:{width:"80%",fontSize:"14px",borderCollapse:"collapse",margin:"20px auto"}},(0,i.yg)("thead",null,(0,i.yg)("tr",null,(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Parameter Name"),(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Required?"),(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Description"))),(0,i.yg)("tbody",null,(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"mlir"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"MLIR file")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"quantize"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Quantization type (F32/F16/BF16/INT8)")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"processor"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},'It depends on the platform being used. The 2024 version of reCamera selects "cv181x" as a parameter.')),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"calibration_table"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"The calibration table path. Required when it is INT8 quantization")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"tolerance"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Tolerance for the minimum similarity between MLIR quantized and MLIR fp32 inference results")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"test_input"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"The input file for validation, which can be an image, npy or npz. No validation will be carried out if it is not specified")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"test_reference"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Reference data for validating mlir tolerance (in npz format). It is the result of each operator")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"compare_all"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Compare all tensors, if set")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"excepts"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Names of network layers that need to be excluded from validation. Separated by comma")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"op_divide"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Try to split the larger op into multiple smaller op to achieve the purpose of ion memory saving, suitable for a few specific models")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"model"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Name of output model file (including path)")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"skip_validation"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Skip cvimodel correctness verification to boost deployment efficiency; cvimodel verification is on by default")))),(0,i.yg)("p",null,"After compilation, a file named ",(0,i.yg)("inlineCode",{parentName:"p"},"yolo11n_1684x_f16.cvimodel")," is generated. The quantized model may have a slight loss in accuracy, but it will be more lightweight and have faster inference speed."),(0,i.yg)("h3",{id:"mlir-to-int8-cvimodel"},"MLIR to INT8 cvimodel"),(0,i.yg)("h4",{id:"calibration-table-generation"},"Calibration table generation"),(0,i.yg)("p",null,"Before converting to the ",(0,i.yg)("strong",{parentName:"p"},"INT8")," model, ",(0,i.yg)("strong",{parentName:"p"},"you need to run calibration to get the calibration table."),"\nThe number of input data is about 100 to 1000 according to the situation.\nThen use the calibration table to generate a symmetric or asymmetric ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel"),". It is gen-\nerally not recommended to use the asymmetric one if the symmetric one already meets the\nrequirements, because the performance of the asymmetric model will be slightly worse than\nthe symmetric model.\nHere is an example of the existing 100 images from ",(0,i.yg)("inlineCode",{parentName:"p"},"COCO2017")," to perform calibration:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"run_calibration \\\n  yolo11n.mlir \\\n  --dataset ../COCO2017 \\\n  --input_num 100 \\\n  -o yolo11n_calib_table\n")),(0,i.yg)("p",null,"After running the command above, a file named ",(0,i.yg)("inlineCode",{parentName:"p"},"yolo11n_cali_table")," will be generated, which\nis used as the input file for subsequent compilation of the ",(0,i.yg)("strong",{parentName:"p"},"INT8")," model."),(0,i.yg)("p",null,"Description of Main Parameters for ",(0,i.yg)("inlineCode",{parentName:"p"},"run_calibration"),":"),(0,i.yg)("table",{style:{width:"80%",fontSize:"14px",borderCollapse:"collapse",margin:"20px auto"}},(0,i.yg)("thead",null,(0,i.yg)("tr",null,(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Parameter"),(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Required?"),(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Description"))),(0,i.yg)("tbody",null,(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"N/A"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the MLIR file")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"dataset"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the input sample directory, where the path contains corresponding images, npz, or npy files")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"data_list"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the sample list; either dataset or data_list must be selected")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"input_num"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the number of calibration samples; if set to 0, all samples are used")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"tune_num"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the number of tuning samples; default is 10")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"histogram_bin_num"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Number of bins for the histogram; default is 2048")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"o"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Output the calibration table file")))),(0,i.yg)("h4",{id:"compile-to-int8-symmetric-quantized-cvimodel"},"Compile to INT8 symmetric quantized cvimodel"),(0,i.yg)("p",null,"After obtaining the ",(0,i.yg)("inlineCode",{parentName:"p"},"yolo11n_cali_table")," file, run the following command to convert it into an ",(0,i.yg)("strong",{parentName:"p"},"INT8")," symmetric quantized model:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"model_deploy \\\n  --mlir yolo11n.mlir \\\n  --quantize INT8 \\\n  --quant_input \\\n  --processor cv181x \\\n  --calibration_table yolo11n_calib_table \\\n  --test_input ../image/dog.jpg \\\n  --test_reference yolo11n_top_outputs.npz \\\n  --customization_format RGB_PACKED \\\n  --fuse_preprocess \\\n  --aligned_input \\\n  --model yolo11n_1684x_int8_sym.cvimodel\n")),(0,i.yg)("p",null,"After compilation, a file named ",(0,i.yg)("inlineCode",{parentName:"p"},"yolo11n_1684x_int8_sym.cvimodel")," is generated. The model quantized to ",(0,i.yg)("strong",{parentName:"p"},"INT8")," is more lightweight and has faster inference speed compared to models quantized to ",(0,i.yg)("strong",{parentName:"p"},"F16/BF16"),"."),(0,i.yg)("h3",{id:"quick-test"},"Quick Test"),(0,i.yg)("p",null,"You can use Node-RED on reCamera for visualization to quickly verify the converted ",(0,i.yg)("inlineCode",{parentName:"p"},"yolo11n_1684x_int8_sym.cvimodel"),". Simply set up a few nodes, as shown in the example video below:"),(0,i.yg)("div",{style:{textAlign:"center"}},(0,i.yg)("iframe",{width:600,height:300,src:"https://www.youtube.com/embed/XdgCt44UR1M",title:"YouTube video player",frameBorder:0,allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:!0})),(0,i.yg)("p",null,"You need to select the ",(0,i.yg)("inlineCode",{parentName:"p"},"yolo11n_1684x_int8_sym.cvimodel")," in the ",(0,i.yg)("inlineCode",{parentName:"p"},"model")," node for quick verification. Double-click the model node, click ",(0,i.yg)("inlineCode",{parentName:"p"},'"Upload"')," to import the quantized model, then click ",(0,i.yg)("inlineCode",{parentName:"p"},'"Done"'),", and finally click ",(0,i.yg)("inlineCode",{parentName:"p"},'"Deploy"'),"."),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:"600",src:"https://files.seeedstudio.com/wiki/reCamera/recamera_model_conversion05.png"})),(0,i.yg)("p",null,"We can view the inference results of the ",(0,i.yg)("strong",{parentName:"p"},"INT8")," quantized model in the ",(0,i.yg)("inlineCode",{parentName:"p"},"preview")," node. The ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel")," obtained through correct conversion and quantization methods is still reliable:"),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:"600",src:"https://files.seeedstudio.com/wiki/reCamera/recamera_model_conversion06.png"})),(0,i.yg)("admonition",{type:"tip"},(0,i.yg)("p",{parentName:"admonition"},"Currently, reCamera's Node-RED only supports preview testing for a limited number of models. In the future, we will adapt more models. If you import a custom model into Node-RED or do not set the specified output tensor as shown in our example, Node-RED's backend does not support preview testing, even if your ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel")," is correct.")),(0,i.yg)("p",null,"We will release tutorials on preprocessing and postprocessing for various models, so you can write your own code to infer your custom ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel"),"."),(0,i.yg)("h3",{id:"mixed-precision-quantization"},"Mixed-Precision Quantization"),(0,i.yg)("p",null,"When the precision of certain layers in a model is easily affected by quantization, but we still need faster inference speed, a single precision quantization may no longer be suitable. In such cases, ",(0,i.yg)("strong",{parentName:"p"},"mixed-precision")," quantization can better address the situation. For layers that are more sensitive to quantization, we can choose ",(0,i.yg)("strong",{parentName:"p"},"F16/BF16")," quantization, while for layers with minimal precision loss, we can use ",(0,i.yg)("strong",{parentName:"p"},"INT8"),"."),(0,i.yg)("p",null,"Next, we will use ",(0,i.yg)("inlineCode",{parentName:"p"},"yolov5s.onnx")," as an example to demonstrate how to quickly convert and quantize the model into a ",(0,i.yg)("strong",{parentName:"p"},"mixed-precision")," ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel"),". Before reading this section, ",(0,i.yg)("strong",{parentName:"p"},"make sure you have gone through the previous sections of the article, as the operations in this section build upon the content covered earlier.")),(0,i.yg)("p",null,"Here is the download link for ",(0,i.yg)("inlineCode",{parentName:"p"},"yolov5s.onnx"),". You can click the link to download the model and copy it to your workspace for further use."),(0,i.yg)("p",null,"Download the model:",(0,i.yg)("br",{parentName:"p"}),"\n",(0,i.yg)("a",{parentName:"p",href:"https://seeedstudio88-my.sharepoint.com/:u:/g/personal/youjiang_yu_seeedstudio88_onmicrosoft_com/EdX8QYfqMnFEvXGUQ-_NjCoBmOalVQNKPWnZpFxcdNchrw?e=KDUkUP"},"Download yolov5s.onnx")),(0,i.yg)("p",null,"After downloading the model, please place it into your ",(0,i.yg)("inlineCode",{parentName:"p"},"workspace")," for the next steps."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"mkdir model_yolov5s && cd model_yolov5s\ncp -rf ${REGRESSION_PATH}/dataset/COCO2017 .\ncp -rf ${REGRESSION_PATH}/image .\nmkdir workspace && cd workspace\n")),(0,i.yg)("p",null,"The first step is still to convert the model to the ",(0,i.yg)("inlineCode",{parentName:"p"},".mlir")," file. Because the precision loss in the YOLO's ",(0,i.yg)("inlineCode",{parentName:"p"},"head")," is minimal when using ",(0,i.yg)("strong",{parentName:"p"},"mixed-precision")," quantization, unlike the previous approach, we will choose the final output name at the end rather than the outputs before the ",(0,i.yg)("inlineCode",{parentName:"p"},"head")," in the ",(0,i.yg)("inlineCode",{parentName:"p"},"--output_names")," parameter.Visualize the ",(0,i.yg)("inlineCode",{parentName:"p"},"ONNx")," in ",(0,i.yg)("a",{parentName:"p",href:"https://netron.app/"},"Netron"),": "),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:"600",src:"https://files.seeedstudio.com/wiki/reCamera/recamera_model_conversion07.png"})),(0,i.yg)("p",null,"Since the normalization parameters of yolov5 are the same as those of yolo11, we can obtain the following command for model_transform:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},'model_transform \\\n  --model_name yolov5s \\\n  --model_def yolov5s.onnx \\\n  --input_shapes [[1,3,640,640]] \\\n  --mean 0.0,0.0,0.0 \\\n  --scale "0.0039216,0.0039216,0.0039216" \\\n  --keep_aspect_ratio \\\n  --pixel_format rgb \\\n  --output_names output \\\n  --test_input ../image/dog.jpg \\\n  --test_result yolov5s_top_outputs.npz \\\n  --mlir yolov5s.mlir\n')),(0,i.yg)("p",null,"Then we also need to generate the calibration table, and this step is the same as in the previous section: "),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"run_calibration \\\n  yolov5s.mlir \\\n  --dataset ../COCO2017 \\\n  --input_num 100 \\\n  -o yolov5s_calib_table\n")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Unlike the section where we converted the int8 symmetric quantized yolo11 model, before executing model_deploy, we need to generate a mixed-precision quantization table"),".** The reference command is as follows:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"run_qtable \\\n  yolov5s.mlir \\\n  --dataset ../COCO2017 \\\n  --calibration_table yolov5s_calib_table \\\n  --processor cv181x \\\n  --min_layer_cos 0.99 \\\n  --expected_cos 0.999 \\\n  -o yolov5s_qtable\n")),(0,i.yg)("p",null,"The parameter description for ",(0,i.yg)("inlineCode",{parentName:"p"},"run_qtable")," is shown in the table below:"),(0,i.yg)("table",{style:{width:"80%",fontSize:"14px",borderCollapse:"collapse",margin:"20px auto"}},(0,i.yg)("thead",null,(0,i.yg)("tr",null,(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Parameter"),(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Required?"),(0,i.yg)("th",{style:{border:"1px solid #000",padding:"8px"}},"Description"))),(0,i.yg)("tbody",null,(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"N/A"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the MLIR file")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"dataset"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the input sample directory, which contains images, npz, or npy files")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"data_list"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the sample list; either `dataset` or `data_list` must be selected")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"calibration_table"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Input calibration table")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"processor"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Yes"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},'It depends on the platform being used. The 2024 version of reCamera selects "cv181x" as a parameter.')),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"fp_type"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the floating-point precision type for mixed precision, supports auto, F16, F32, BF16; default is auto")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"input_num"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the number of input samples; default is 10")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"expected_cos"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the minimum expected cosine similarity for the final network output layer; default is 0.99")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"min_layer_cos"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the minimum cosine similarity for each layer's output; values below this threshold will use floating-point computation; default is 0.99")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"debug_cmd"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify debugging command string for development use; default is empty")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"global_compare_layers"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the layers to replace for final output comparison, e.g., 'layer1,layer2' or 'layer1:0.3,layer2:0.7'")),(0,i.yg)("tr",null,(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"loss_table"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"No"),(0,i.yg)("td",{style:{border:"1px solid #000",padding:"8px"}},"Specify the file name to save the loss values of all layers quantized to floating-point types; default is full_loss_table.txt")))),(0,i.yg)("p",null,"After each layer's predecessor layer is converted to the corresponding floating-point mode based on its ",(0,i.yg)("inlineCode",{parentName:"p"},"cos"),", the ",(0,i.yg)("inlineCode",{parentName:"p"},"cos")," value calculated for that layer is checked. If the cos is still smaller than the ",(0,i.yg)("inlineCode",{parentName:"p"},"min_layer_cos")," parameter, the current layer and its direct successor layers will be set to use floating-point operations."),(0,i.yg)("p",null,(0,i.yg)("inlineCode",{parentName:"p"},"run_qtable")," recalculates the cos of the entire network's output after setting each pair of adjacent layers to use floating-point computation. If the cos exceeds the specified ",(0,i.yg)("inlineCode",{parentName:"p"},"expected_cos")," parameter, the search terminates. Therefore, setting a larger ",(0,i.yg)("inlineCode",{parentName:"p"},"expected_cos")," will result in more layers being attempted for floating-point operations."),(0,i.yg)("p",null,"Finally, run ",(0,i.yg)("inlineCode",{parentName:"p"},"model_deploy")," to obtain the ",(0,i.yg)("strong",{parentName:"p"},"mixed-precision")," ",(0,i.yg)("inlineCode",{parentName:"p"},"cvimodel"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"model_deploy \\\n  --mlir yolov5s.mlir \\\n  --quantize INT8 \\\n  --quantize_table yolov5s_qtable \\\n  --calibration_table yolov5s_calib_table \\\n  --customization_format RGB_PACKED \\\n  --fuse_preprocess \\\n  --aligned_input \\\n  --processor cv181x \\\n  --model yolov5s_mix-precision.cvimodel\n")),(0,i.yg)("p",null,"After obtaining ",(0,i.yg)("inlineCode",{parentName:"p"},"yolov5s_mix-precision.cvimodel"),", we can use ",(0,i.yg)("inlineCode",{parentName:"p"},"model_tool")," to view detailed information about the model:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"model_tool --info yolov5s_mix-precision.cvimodel\n")),(0,i.yg)("p",null,"Key information such as ",(0,i.yg)("inlineCode",{parentName:"p"},"TensorMap")," and ",(0,i.yg)("inlineCode",{parentName:"p"},"WeightMap")," will be printed to the terminal:"),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:"500",src:"https://files.seeedstudio.com/wiki/reCamera/recamera_model_conversion08.png"})),(0,i.yg)("p",null,"We can run an example in reCamera to verify the mixed-precision quantized YOLOv5 model. Pull the compiled test example:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/jjjadand/yolov5_Test_reCamera.git\n")),(0,i.yg)("p",null,"Copy the compiled examples and ",(0,i.yg)("inlineCode",{parentName:"p"},"yolov5s_mix-precision.cvimodel"),"using software like ",(0,i.yg)("a",{parentName:"p",href:"https://filezilla-project.org/"},"FileZilla")," to reCamera. (You can review ",(0,i.yg)("a",{parentName:"p",href:"https://wiki.seeedstudio.com/recamera_getting_started/"},"Getting Started with reCamera"),")"),(0,i.yg)("p",null,"After the copy is complete, ",(0,i.yg)("strong",{parentName:"p"},"run the command in the recamera terminal:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"cp /path/to/yolov5s_mix-precision.cvimodel /path/to/yolov5_Test_reCamera/solutions/sscma-model/build/\ncd yolov5_Test_reCamera/solutions/sscma-model/build/\nsudo ./sscma-model yolov5s_mix-precision.cvimodel Dog.jpg Out.jpg\n")),(0,i.yg)("p",null,"Preview ",(0,i.yg)("inlineCode",{parentName:"p"},"Out.jog"),", the ",(0,i.yg)("inlineCode",{parentName:"p"},"mixed-precision")," quantized yolov5 model inference results are as follows:"),(0,i.yg)("div",{align:"center"},(0,i.yg)("img",{width:"500",src:"https://files.seeedstudio.com/wiki/reCamera/yolov5Out.jpg"})),(0,i.yg)("h2",{id:"resources"},"Resources"),(0,i.yg)("p",null,(0,i.yg)("a",{parentName:"p",href:"https://github.com/Seeed-Studio/reCamera-OS"},"reCamera OS")),(0,i.yg)("p",null,(0,i.yg)("a",{parentName:"p",href:"https://github.com/Seeed-Studio/OSHW-reCamera-Series"},"reCamera Series")),(0,i.yg)("p",null,(0,i.yg)("a",{parentName:"p",href:"https://github.com/Seeed-Studio/sscma-example-sg200x"},"reCamera example")),(0,i.yg)("h2",{id:"tech-support--product-discussion"},"Tech Support & Product Discussion"),(0,i.yg)("p",null,"Thank you for choosing our products! We are here to provide you with different support to ensure that your experience with our products is as smooth as possible. We offer several communication channels to cater to different preferences and needs."),(0,i.yg)("div",{class:"button_tech_support_container"},(0,i.yg)("a",{href:"https://forum.seeedstudio.com/",class:"button_forum"}),(0,i.yg)("a",{href:"https://www.seeedstudio.com/contacts",class:"button_email"})),(0,i.yg)("div",{class:"button_tech_support_container"},(0,i.yg)("a",{href:"https://discord.gg/eWkprNDMU7",class:"button_discord"}),(0,i.yg)("a",{href:"https://github.com/Seeed-Studio/wiki-documents/discussions/69",class:"button_discussion"})))}y.isMDXComponent=!0}}]);